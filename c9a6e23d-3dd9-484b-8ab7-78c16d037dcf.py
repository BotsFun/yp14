#!/usr/bin/env python
# coding: utf-8

# # Банки — Сегментация пользователей по потреблению

# В «Метанпромбанк», один из лучших банков страны, требуется аналитик с уверенным владением Python. Вашими главными задачами станут анализ оттока клиентов и сегментирование пользователей банка. Сегментация покажет, как клиенты пользуются нашими услугами.
# 
# ### Задача**:**
# 
# Проанализировать клиентов регионального банка и сегментировать пользователей по количеству потребляемых продуктов.
# 
# - Проводим исследовательский анализ данных,
# - Сегментируем пользователей на основе данных о количестве потребляемых продуктов,
# - Сформулируем и проверим статистические гипотезы.
# 
#     *Проверьте гипотезу различия дохода между теми клиентами, которые пользуются двумя продуктами банка, и теми, которые пользуются одним.*
# 
#     *Сформулируйте и проверьте статистическую гипотезу относительно представленных данных.*

# ### **Описание данных:**
# 
# Датасет содержит данные о клиентах банка «Метанпром». Банк располагается в Ярославле и областных городах: Ростов Великий и Рыбинск.
# 
# Колонки:
# 
# - `userid` — идентификатор пользователя,
# - `score` — баллы кредитного скоринга,
# - `City` — город,
# - `Gender` — пол,
# - `Age` — возраст,
# - `Objects` — количество объектов в собственности,
# - `Balance` — баланс на счёте,
# - `Products` — количество продуктов, которыми пользуется клиент,
# - `CreditCard` — есть ли кредитная карта,
# - `Loyalty` — активный клиент,
# - `estimated_salary` — заработная плата клиента,
# - `Churn` — ушёл или нет.

# ## Оглавление
# 1. [Шаг 1. Откроем файлы с данными и изучим общую информацию](#step1)  
#     1.1 [Вывод к Шагу 1](#step1.1)
#     
#         
# 2. [Шаг 2. Предобратка данных и исследовательский анализ данных (EDA)](#step2)  
#     2.1  [Обработка дубликатов](#step2.1)  
#     2.2  [Обработка пропущенных значений](#step2.2)  
#     2.3  [Столбчатые гистограммы и распределение признаков](#step2.3)  
#     2.4  [Матрица корреляции](#step2.4)  
#     2.5  [Вывод к Шагу 2](#step2.5)  
#   
#     
# 3. [Шаг 3. Сегментирование пользователей на основе данных о количестве потребляемых продуктов](#step3)   
#     3.1   [Алгоритм K-Means](#step3.1)  
#     3.2   [Средние значения признаков для кластеров](#step3.2)    
#     3.3   [Матрица корреляции](#step3.3)    
#     
#     
#   
# 4. [Шаг 4. Сформулируем и проверим статистические гипотезы](#step4)   
#     4.1   [Проверим гипотезу различия дохода между теми клиентами, которые пользуются двумя продуктами банка, и теми, которые пользуются одним.](#step4.1)  
#     4.2   [Проверим статистическую гипотезу относительно представленных данных](#step4.2)  
#    
#     
#  
# 5. [Шаг 5. Общий вывод.](#step5)  

# ### Коментарий наставника
# 
# <font color = blue>Георгий, привет! <br> Спасибо за задание. <br>В этот раз нужно было только прислать план проекта, по которому его планируется делать. Круто, что уже готово остальное, но внимательно проверить я это смогу после выполнения А/Б теста и sql. Сейчас все выглядит неплохо. Пока только замечу, что значение churn=1 значит, что клиент утек. Поэтому надо будет пересмотреть некоторые выводы. Например, что женщины чаще остаются. На самом деле наоборот.</font>

# <div class="alert alert-info">
# <h2> Комментарий студента v0</h2>
# 
# Исправил, выводы переписал, презентацию обновил :)  
#     
# Получается мы потеряли женщин Бальзаковского возраста :D 
# 
# </div>

# ### Коментарий наставника
# 
# <font color = blue>Привет!
# <br>Большая часть задачи явно готова. Но есть вещи, которые нужно доработать. Про них есть комментарии ниже
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Привет!
#     
# По идее все замечания исправил, дашборд дополнил, презентацию обновил. К каждому пункту старался оставить комментарий
# 
# </div>

# ### Коментарий наставника
# 
# <font color = purple>Спасибо за лоработки. Получился отличный проект. Поздравляю с его завершением
# </font>

# ### Шаг 1. Откроем файлы с данными и изучим общую информацию <a id="step1"></a>   
# </div>

# In[1]:


import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from scipy import stats as st
import math
import numpy as np
from functools import reduce
import seaborn as sns
import datetime as dt
import scipy.stats as stats
from plotly import graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import KMeans

data = pd.read_csv('/datasets/bank_dataset.csv')

display(data.head(10))
data.info()


# ### Вывод<a id="step1.1"></a>   
# </div>  
# 
# Мы получили датасет с 10000 записями. В столбце `Balance` есть пропущенные значения. К форматам особых претензий нет. Обязательно нужно проверить дубликаты, но с таким датасетом должно быть не сложно работать.
# 
# Также некоторые проблемы нам могут принести названия столбцов, поэтому с их изменеия и начнём следующий этап. 

# ### Коментарий наставника
# 
# <font color = blue>Отлично. План понятен
# </font>

# ### Шаг 2. Предобратка данных и исследовательский анализ данных (EDA) <a id="step2"></a>   
# </div>  

# Приведём все к нижнему регистру

# In[2]:


data.columns = data.columns.str.lower()
display(data.columns)


# Посмотрим нет ли каких-то артефактов в ряде стобцов

# In[3]:


display(data[['city','gender','products','creditcard','loyalty', 'churn']].apply(lambda x: x.value_counts()).T.stack())


# Кажется ничего лишнего, идем дальше и проверим наличие полных дубликатов

# ### Обработка дубликатов <a id="step2.1"></a>   
# </div>  

# In[4]:


data.duplicated().sum()


# Создадим дополнительный столбец для проверки неполных дубликатов

# In[5]:


data['dupl'] = data['score'].astype(str) + data['userid'].astype(str)

display(data.head(5))


# In[6]:


data['dupl_2'] = data['score'].astype(str) + data['city'].astype(str) +                     data['gender'].astype(str) + data['age'].astype(str) +                     data['objects'].astype(str) + data['products'].astype(str) +                     data['creditcard'].astype(str) + data['loyalty'].astype(str) +                     data['churn'].astype(str) + data['estimated_salary'].astype(str)
print(data.head(5))
print('Количество неполных дубликатов', data['dupl_2'].duplicated().sum())


# In[7]:


print('Количество неполных дубликатов', data['dupl'].duplicated().sum())
print('Количество дубликатов в userid', data['userid'].duplicated().sum())


# In[ ]:





# ### Коментарий наставника
# 
# <font color = blue>Тут я не до конца понял логику создания столбца. Кажется, что если среди userid нет дублей, то и среди dupl их не будет. Если хочется прямо досконально проверить датасет на дубликаты, можно предположить, что сюда попали клиенты с разными id, но полностью совпадающими остальными атрибутами
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Согласен, логика потерялась, но я, честно говоря, так и сделал, как ты говоришь, постепенно добавлял столбцы и смотрел как меняется количество дубликатов
# 
# </div>

# ### Коментарий наставника
# 
# <font color = purple>Можно на досуге подумать, как это автоматизировать. И в будущем всегда одну и туже функцию применять
# </font>

# С дубликатами проблем нет, поэтому удалим наш временный столбец `dupl`

# In[8]:


data = data.drop(['dupl','dupl_2'], axis = 1)
display(data.head(5))


# ### Обработка пропущенных значений <a id="step2.2"></a>   
# </div>  

# Теперь разберёмся с пропущенными значениями в `balance`. Их достаточно много, около **36%**, поэтому просто избавиться мы от них не можем

# In[9]:


print(data['balance'].isnull().sum())


# Прежде, чем работать с `balance` попробуем найти возможность отсеять часть значений. Проверим количество людей с нулевым количеством объектов и качество таких строк

# In[10]:


data['objects'].value_counts()


# In[11]:


display(data.loc[data['objects'] == 0])


# К сожалению, здесь мы ничего сделать не cможем, эти данные также влияют на исследование.  
# 
# Поэтому попробуем сгруппировать наш датасет по разным столбцам.

# In[12]:


for column in ['score','city','gender','objects','products','creditcard','loyalty','churn']:
    print('Группировка по ' + column)
    display(data.groupby(column)['balance'].mean())
    print()


# И тут мимо, группировка не даёт нам каких-то инсайтов.
# Создавать просто 37% синтетических данных **нет необходимости**, так как, скорее всего, это не повлияет на выводы наше исследования.
# 
# Таким образом принимаем решение ничего не делать с пропущенными значениями в столбце `balance`

# ### Коментарий наставника
# 
# <font color = blue>С пропусками поступил абсолютно правильно
# </font>

# In[13]:


data.head(5)


# ### Столбчатые гистограммы и распределение признаков <a id="step2.3"></a>   
# </div> 

# Визуализируем возвращаемость клиентов по ряду качественных столбцов

# In[14]:


for column in ['gender','objects','products','creditcard','loyalty','city']:
    plt.figure(figsize=(5, 5))
    sns.countplot(x = data[column], hue='churn', data=data)
    plt.title(column)
    plt.show()


# Для количественных построим ящик с усами

# In[15]:


for column in ['score', 'age', 'balance', 'estimated_salary']:
    plt.figure(figsize=(5, 5))
    sns.boxplot(x = 'churn', y = data[column], data = data)
    plt.title('Ящик с усами' + ' ' + column)
    plt.show()


# ### Матрица корреляции <a id="step2.4"></a>   
# </div> 

# Для полного успокоения построим матрицу корреляции. 

# In[16]:


cm = data.corr()
plt.figure(figsize=(15, 10))
sns.heatmap(cm, annot = True, fmt = '0.2f')
plt.show()


# ### Вывод к Шагу №2 <a id="step2.5"></a>   
# </div>  

# Каких-то интересных выводов из графиков сделать не предоставляется возможным, все в пределах логики (кроме женщин). 
# - Возвращаемость клиентов не зависит от текущего баланса, уровня зарплаты или кредитного скоринга;
# - А вот возраст влияет, чем младше, тем меньше вероятность, что клиент уйдёт от нас в другой банк;
# - Женщины к нашему банку менее лояльны, чем мужчины;
# - Больше всего наших клиентов в **Ярославле**. Это и понятно - он самый крупный город из анализируемых;
# - По соотношению уходящих клиентов лучше остальных показывает себя **Рыбинск**, при населении около 200 тысяч человек, отток в этом городе самый маленький;
# - По количеству клиентов на душу населения **Ростов Великий** выбивается вперёд, но и отток клиентов самый большой;
# - Количество объектов в собственности также принципиально не влияет на возвращаемость.
# - Лучше всего показывают себя пользователи с двумя продуктами, а вот отток у пользователей с одним продуктом и, наоборот, >=3 значительнее.
# 
# Матрица корреляции в целом подтверждает выводы выше, но обнаруживает ещё один факт
# - Активные клиенты с большим доверием относятся к нашему банку. Возможно, им нравится наша скорость обработки их запросов. 
# 

# ### Коментарий наставника
# 
# <font color = blue>Можно было еще заметить, что лояльность довольно сильно влияет на отток. И совсем ничего не сказано про города
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Добавил аналитику по городам, внёс дополнения к выводу
# </div>

# ### Коментарий наставника
# 
# <font color = purple>Спасибо. Стало лучше)
# </font>

# ### Шаг 3. Сегментирование пользователей на основе данных о количестве потребляемых продуктов <a id="step3"></a>   
# </div>  

# Соберем `clusters` на основании сводной таблицы.

# In[17]:


display(data)


# In[18]:


clusters = data

clusters = clusters.drop(['userid','city', 'gender','balance'], axis = 1)
clusters


# In[ ]:





                        


# ### Коментарий наставника
# 
# <font color = red>Тут ошибка. Кластеризация на средних не делается. Среднее "убивает" практически всю информацию о данных. Модель бессмысленна. Нужно на общем датасете сделать и пересмотреть выводы (не забудь удалить userid перед этим)
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Убрал кластеризацию на средних, сделал на общем датасете
#    
#     
# </div>

# In[ ]:





# Стандартизируем данные

# In[19]:


scaler = StandardScaler() 
x_sc = scaler.fit_transform(clusters)


# Применим функцию `linkage()` c параметром method = `ward`. 

# In[20]:


linked = linkage(x_sc, method = 'ward')


# Построим дендрограмму

# In[21]:


plt.figure(figsize=(16, 9))  
dendrogram(linked, orientation='top')
plt.title('Дендрограмма')
plt.show()


# Как видно из дендрограммы мы можем кластеризировать пользователей на 4 кластера.

# ### Алгоритм K-Means <a id="step3.1"></a>   
# </div>  

# In[22]:


km = KMeans(n_clusters = 4)


# Прогнозируем кластеры для наблюдений

# In[23]:


labels = km.fit_predict(x_sc)


# Cохраняем метки кластера в поле нашего датасета

# In[24]:


clusters['cluster_churn'] = labels


# ### Средние значения признаков для кластеров <a id="step3.2"></a>   
# </div>  

# Сгруппируем данные по `cluster_churn`. Округлим данные до двух знаков после запятой и перевернём таблицу для удобства воспрития

# In[25]:



display(clusters.groupby('cluster_churn').mean().round(2).T)


# - Средний возраст **Кластеров 1,2,3** меньше чем **Кластера 0**;
# - Средний доход **Кластера 0** чуть выше, чем у хороших кластеров;
# - Кредитный скоринг выше у хороших кластеров (не зря сотрудники работают);
# - Количество объектов и продуктов не так сильно влияет на возвращаемость, хотя для проблемного **Кластера №0** наблюдается снижение количества продуктов;
# - Для **кластера №0** (на который нам не стоит ориентироваться) характерна активность ниже среднего, высокий средний возраст;
# - **Кластер 3** отличается высокой активностью, наличием кредитной карты;
# - **Кластер 1** также имеет кредитную карту, но не проявляет особой активности;
# - **Кластер 2** наоборот не имеет кредитов, но имеет средний уровень активности.

# ### Коментарий наставника
# 
# <font color = purple>Вот теперь результат кластеризации больше похож на правду. Получилось хорошее разделение.
# <br><br> На самом деле нужно еще уметь оценивать качество модели. Но для этого уже полее глубокие знания по ML нужны
# </font>

# ### Шаг 4. Сформулируем и проверим статистические гипотезы. <a id="step4"></a>   
# </div>  

# ### Коментарий наставника
# 
# <font color = red>Тут нужно обосновать, почему тот или иной тип теста выбирался. От чего это зависело?
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Дополнил ниже информацией 
# В том числе для этого посчитал дисперсию для одного и двух продуктов. 
# 
# Для нас важно подчиняется ли нормальному распределению, являются ли зависимыми и количество групп. На основании этого и применяем методику. 
#    
#     
# </div>

# ### Проверим гипотезу различия дохода между теми клиентами, которые пользуются двумя продуктами банка, и теми, которые пользуются одним. <a id="step4.1"></a>   
# </div>  

# Для решения данной задачи применим t-распределение Стьюдента. У нас две независимых группы (с одним и двумя продуктами соответственно). Поэтому `st.ttest_ind` нам прекрасно подходит. 
# 
# Нулевая гипотеза H<sub>0</sub> - Доход пользователей, которые пользуются **двумя продуктами**, *не отличается* от дохода пользователей, который пользуются **одним продуктом**   
# Альтернативная гипотеза H<sub>1</sub> - Доход пользователей, которые пользуются **двумя продуктами**, *отличается* от дохода пользователей, который пользуются **одним продуктом** 

# In[26]:


one_product = data.query('products == 1')['estimated_salary']
two_products = data.query('products == 2')['estimated_salary']
print('Дисперсия для пользователей с двумя продуктами:', np.var(two_products))
print('Дисперсия для пользователей с одним продуктом:', np.var(one_product))


alpha = 0.05

results = st.ttest_ind(
    two_products,
    one_product, 
    equal_var=True)

print('p-значение:', results.pvalue)

if (results.pvalue < alpha):
    print("Отвергаем нулевую гипотезу")
else:
    print("Не получилось отвергнуть нулевую гипотезу")


# Таким образом мы не можем подтвердить, что доход напрямую связан с количеством используемых продуктов

# ### Проверим статистическую гипотезу относительно представленных данных <a id="step4.2"></a>   
# </div>  

# Для проверки статистической гипотезы используем непараметрический подход. В частности будем использовать метод Манна-Уитни - `st.mannwhitneyu`. 
# 
# Ключевая идея — проранжировать две выборки по порядку от меньшего к большему и сравнить ранги одних и тех же значений, попавших в обе выборки. Ранг — это место в упорядоченной выборке.  Разница между рангами одних и тех же значений может одинакова, и такой сдвиг называют типичным. Значит, просто добавились значения, сдвинувшие все остальные.

# Нулевая гипотеза H<sub>0</sub> - Предоставленные данные имеют статистически значимую разницу.    
# Альтернативная гипотеза H<sub>1</sub> - Предоставленные данные не имеют статистически значимой разницы.   

# In[27]:


alpha = 0.05 # критический уровень статистической значимости

results = st.mannwhitneyu(one_product, two_products)

print('p-значение: ', results.pvalue)

if (results.pvalue < alpha):
    print("Отвергаем нулевую гипотезу: разница статистически значима")
else:
    print("Не получилось отвергнуть нулевую гипотезу, вывод о различии сделать нельзя")


# ### Коментарий наставника
# 
# <font color = purple>Проверка гипотез выполнена правильно
# </font>

# ### Шаг 5. Общий вывод <a id="step5"></a>   
# </div>  

# В процессе работы над проектом мы обнаружили большое количество пропущенных значений, но не стали создавать лишние синтетические данные, чтобы не нарушать логику датасета.  
# 
# В остальном претензий к полученному датасету у нас нет, на поставленную задачу пропущенные значения не влияли.
# 
# В ходе исследования мы выяснили, что:  
# 
# - Женщины меньше любят наш банк и чаще уходят;
# - Чем старше клиент, тем хуже для банка;
# - В банке, в целом, хорошо справляются с активными клиентами;
# - Принципиально можно выделить 2 сегмента клиентов (с возможностью разбивки нужного нам кластера на 3 подгруппы соответственно, отличающихся наличием кредиток и активностью);
# - Для "плохого" кластера характерен высокий возраст, несколько более низкое количество используемых продуктов и низкий кредитный скоринг.
# - В ходе исследования не удалось опровергнуть гипотезу о том, что доход связан с количеством используемых продуктов;
# - Датасет не имеет статистических выбросов.
# 
# #### Со своей стороны мы бы рекомендовали усилить привлечение мужской аудитории. Возможно, что есть пути привлечения и женской части клиентов, но для этого в коммуникацию нужно добавить "нежности" и "спокойствия", не потеряв при этом активных клиентов. Также существует перекос в сторону в 30-40 летних клиентов, а поколение старше чаще уходит (возможно, нужно опять же добавить "спокойствия"). 
# #### С клиентами, у которых всего 1 продукт банка, нужно работать в части расширения продуктовой линейки в их портфеле, так вероятность их отказа от дальнейшего сотрудничества падает.  
# 
# 

# ### Коментарий наставника
# 
# <font color = blue>С этими выводами согласен. Нужно будет их обогатить после переделанной кластеризации
# </font>

# ### Коментарий наставника
# 
# <font color = purple>Отличный результат
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# Обогатил выводы переделанной кластеризацией :)
# </div>

# Презентация: <https://yadi.sk/i/ux6ehkjAA5ey8Q>   
# Дашборд: <https://public.tableau.com/profile/georgy.samulenkov#!/vizhome/DashboardYD14/Dashboard1?publish=yes> 
# 
# 

# ### Коментарий наставника
# 
# <font color = blue>Презентация хорошая. Правильно оформлена, все выводы на месте. Такие обычно нравятся заказчикам. Видно, что на нее не ушло много времени, а значит основной ресурс пошел в аналитику. 
#     <br><br>Для дашборда не очень удачно выбран фильтр с оттоком. Я бы предложил списком его сделать. И можно было бы побольше фильтров еще для графика добавить
# </font>

# <div class="alert alert-info">
# <h2> Комментарий студента</h2>
# 
# По дашборду замечания учёл, фильтров добавил! В презентации информацию обновил
# </div>

# ### Коментарий наставника
# 
# <font color = purple>Принимается :) 
#     <br><br>Фильтр по возрасту только нужно было разбить на категории. Сейчас очень много нажатий нужно, чтобы нужное отобрать
# </font>

# In[ ]:




